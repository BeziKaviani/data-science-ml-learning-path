# Machine Learning & Data Science Mastery Path üöÄ

![Status](https://img.shields.io/badge/Status-In%20Progress-blueviolet)
![Python](https://img.shields.io/badge/Python-3.x-3776AB?logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?logo=numpy&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?logo=scikit-learn&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

A comprehensive, structured, and practical journey through the core and advanced concepts of Machine Learning and Data Science. This repository serves as my personal knowledge base, documenting my progress, code, and projects from foundational data manipulation to deploying production-ready ML systems.

## üó∫Ô∏è Learning Roadmap & Progress

The journey is structured into the following modules, to be completed sequentially:

| Module | Topics Covered | Status |
|:-------|:---------------|:-------|
| **01** | **Foundational Data Manipulation:** Mastery of NumPy & Pandas for efficient data handling. | ‚úÖ **Completed** |
| **02** | **Descriptive Statistics:** Understanding data distributions, central tendency, and variability. | ‚úÖ **In Progress** |
| **03** | **Inferential Statistics & Hypothesis Testing:** Drawing conclusions from data and validating assumptions. | ‚åõ Planned |
| **04** | **Data Visualization & Storytelling:** Creating insightful plots and dashboards with Matplotlib, Seaborn, and Plotly. | ‚åõ Planned |
| **05** | **Data Preprocessing & Wrangling:** Cleaning, normalization, encoding, and feature scaling for ML readiness. | ‚åõ Planned |
| **06** | **Basic Classification & Regression Algorithms:** Hands-on with Linear Models, K-NN, Decision Trees, and SVM. | ‚åõ Planned |
| **07** | **Advanced Supervised Learning:** Ensemble methods (Random Forest, GBM, XGBoost), Hyperparameter tuning, and model evaluation. | ‚åõ Planned |
| **08** | **Unsupervised Learning:** Clustering algorithms like K-Means, DBSCAN, and Hierarchical Clustering. | ‚åõ Planned |
| **09** | **Outlier Detection:** Identifying and handling anomalies in datasets. | ‚åõ Planned |
| **10** | **Dimensionality Reduction:** Techniques like PCA and t-SNE for feature extraction and visualization. | ‚åõ Planned |
| **11** | **Deep Learning Fundamentals:** Introduction to Neural Networks, CNNs, and RNNs using TensorFlow/Keras. | ‚åõ Planned |
| **12** | **Advanced Deep Learning:** Transfer Learning, Autoencoders, and Introduction to Transformer architectures. | ‚åõ Planned |
| **13** | **ML Software Development:** Building, packaging, and distributing ML applications. | ‚åõ Planned |
| **14** | **MLOps Foundations:** Version control (Git), containerization (Docker), and cloud storage (S3) for ML. | ‚åõ Planned |
| **15** | **Data & Workflow Management:** Orchestrating end-to-end ML pipelines and managing data workflows. | ‚åõ Planned |

## üöÄ Current Focus: Module 02 - Descriptive Statistics

I am currently working through the second module, deepening my understanding of:
-   **Measures of Central Tendency:** Mean, median, mode, and their applications
-   **Measures of Variability:** Range, variance, standard deviation, and IQR
-   **Data Distributions:** Normal distribution, skewness, kurtosis, and probability density functions
-   **Correlation Analysis:** Pearson, Spearman, and Kendall correlation coefficients
-   **Practical Implementation:** Applying statistical concepts using Python and real-world datasets

The code and analysis for this module can be found in the [`/02-descriptive-statistics`](./02-descriptive-statistics) directory.


## üõ†Ô∏è Tech Stack

*   **Languages:** Python
*   **Core Libraries:** NumPy, Pandas, SciPy, Scikit-Learn, Statsmodels
*   **Visualization:** Matplotlib, Seaborn, Plotly
*   **Deep Learning:** TensorFlow, Keras
*   **Development:** Jupyter Lab, VS Code, Git
*   **Deployment & MLOps:** Docker, AWS S3, Streamlit/FastAPI (Planned)

## üå± Purpose

This repository is built to:
*   **Learn by Doing:** Translate theoretical knowledge into practical code
*   **Build Publicly:** Showcase my structured learning path and progress to the community
*   **Create a Reference:** Develop a comprehensive resource I can refer back to
*   **Prepare for Production:** Bridge the gap between academic concepts and real-world ML application

---

<div align="center">
<sub><sup>Built with a passion for turning data into insights.</sup></sub>
</div>
